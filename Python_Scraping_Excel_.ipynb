{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8c1DZ3Am5qgPegEyoEBx5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> Python Web Scraping and Excel as a Competitive Advantage</center></h1>\n",
        "\n",
        "<center><img width=\"50%\" src=\"https://raw.githubusercontent.com/GabrielSOliveir/Images_articles/main/7591474.jpg\"></center>\n",
        "\n",
        "In today's world, data collection is an essential task for many companies and professionals who use various information, such as website URLs, for market research, competitive analysis, and customer prospecting. However, manually performing this task can be time-consuming and labor-intensive, especially when it comes to collecting data from multiple sources on the web. That's where web scraping comes in, a technique that allows for the automatic extraction of information from the internet. In this case, the Python programming language is a popular choice due to its simplicity, ease of use, and powerful libraries.\n",
        "\n",
        "In this article, we will explore how Python can be used to optimize data collection for Excel using web scraping, highlighting its advantages, the use of keywords to filter search results, and the importance of well-planned code. Additionally, we will see how the use of Python can save time and money in data collection tasks.\n",
        "\n",
        "One of the main advantages of using Python for web scraping is its simplicity and ease of use. With popular libraries like Beautiful Soup and Requests, it is possible to extract data from web pages and manipulate them efficiently, even for users with little programming knowledge. This makes the process more accessible and enables professionals from various fields to use this technique to collect relevant data.\n",
        "\n",
        "A practical example where the use of Python for web scraping can be extremely useful is in collecting company URLs based solely on their names. Recently, we faced a challenging task of collecting the URLs of approximately 1200 companies.\n",
        "\n",
        "<center><img width=\"100%\" src=\"https://raw.githubusercontent.com/GabrielSOliveir/Images_articles/main/Screenshot%202023-04-14%20at%2019.59.34.png\"></center>\n",
        "\n",
        "First, we structured the problem and thought about possible obstacles that could arise, such as websites with similar names, companies without official websites, and others that may have undergone mergers or name changes.\n",
        "\n",
        "Using Python, we were able to develop efficient code that automated the Google search process, with each step of the code carefully planned to ensure efficiency and accuracy in obtaining the desired results.\n",
        "\n",
        "Another important aspect in optimizing the web scraping process was the use of relevant keywords. By using strategic keywords, it was possible to filter search results and obtain only the desired data, making the process more efficient and economical in terms of time and resources. Additionally, we were able to reduce the number of companies with similar names and even news articles reporting on mergers and companies that changed their names. Careful selection of keywords is crucial to ensure that only relevant information is extracted and that the data collection process is optimized.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gzbldmdN3Ir_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install googlesearch-python\n",
        "import pandas as pd\n",
        "from googlesearch import search\n",
        "# Carrega o arquivo Excel\n",
        "df = pd.read_excel('/content/Book2.xlsx')\n",
        "# Cria uma nova coluna para armazenar as URLs dos sites\n",
        "df['URL do Site'] = ''\n",
        "df.head()"
      ],
      "metadata": {
        "id": "sJ_fOPJfN5D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Excel file\n",
        "df = pd.read_excel('exemple.xlsx')\n",
        "# Creating keywords\n",
        "keywords = ['fiber optic internet']\n",
        "# Iterates through the companies in the 'Company Name' column\n",
        "for i, company in enumerate(df['Company Name']):\n",
        "      print(f'Searching the website of the company {company}...')\n",
        "\n",
        "      # Do Google search by adding as keywords\n",
        "      query = f'{company} {\"|\".join(keywords)} site'\n",
        "      for j, url in enumerate(search(query, num_results=1)):\n",
        "          # Copy the first URL found to the 'Site URL' column\n",
        "          df.at[i, 'Site URL'] = url\n",
        "          print(f'Website found: {url}')\n",
        "          break"
      ],
      "metadata": {
        "id": "O-4ourmr-DFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result in a new Excel file\n",
        "df.to_excel('companies_with_urls.xlsx', index=False)\n",
        "print('File saved with the URLs of the sites found.')"
      ],
      "metadata": {
        "id": "jY8AISVqR2zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img width=\"100%\" src=\"https://raw.githubusercontent.com/GabrielSOliveir/Images_articles/main/Screenshot%202023-04-14%20at%2020.06.45.png\"></center>\n",
        "\n",
        "As a result, it is possible to obtain a new spreadsheet with each URL correctly associated with the company.\n",
        "<center><img width=\"100%\" src=\"https://raw.githubusercontent.com/GabrielSOliveir/Images_articles/main/Screenshot%202023-04-14%20at%2020.03.23.png\"></center>\n",
        "\n",
        "\n",
        "Finally, automating the data collection process with Python also results in time and cost savings. Through automation, tasks that would be time-consuming if done manually can be completed in a fraction of the time, allowing for more efficient use of available resources. Additionally, the accuracy of results obtained with automation is higher, minimizing human errors and the need for rework. In summary, the use of Python for web scraping can significantly optimize the data collection process, providing advantages such as simplicity, ease of use, and keyword utilization. Therefore, it can be a valuable option for optimizing data collection in various projects."
      ],
      "metadata": {
        "id": "McLU-Nf8CzHD"
      }
    }
  ]
}